---
title: "EDA and Collaborative Filtering"
author: "Brandon Hoeft"
date: "October 26, 2017"
output: # http://rmarkdown.rstudio.com/markdown_document_format.html
  md_document:
    variant: markdown_github
    toc: TRUE
    toc_depth: 4
---

```{r setup, include = FALSE}
# my global options defined for each code chunk.
knitr::opts_chunk$set(fig.width=8, fig.height=6, echo=FALSE, warning=FALSE, message=FALSE, comment = '')
```

## Introduction

This Markdown file has most of my code, relevant plots, and explanations of decisions made so far in the data wrangling process.

I will update it with collaborative filtering model information too, as needed. 

## Getting the data into R

I used the following packages to do most of the initial data pre-processing and exploratory analysis. I'm working on an AWS AMI instance that's a linux machine with RStudio and git installed, using one of the instances publicly available and pre-configured from [Louis Aslett's website](http://www.louisaslett.com/RStudio_AMI/). I initially started with the free tier, *t2.micro* instance, but once I started the modeling phase, I scaled up to *m4.large*, which is also a general purpose computing instance but with better RAM, CPU, and better optimized memory management. AWS AMI instance costs can be calculated [here](https://calculator.s3.amazonaws.com/index.html). 

The datasets queried by Matt Hayden, I've put in my own S3 Bucket on Amazon Web Services (AWS). As such, the `aws.s3` [package](https://github.com/cloudyr/aws.s3) was used to access that bucket and bring the datasets into my environment. Access Keys are needed to make read/write calls to the bucket, but I've intentionally left that code out of here. 

``` {r echo = TRUE}

library(aws.s3) 
library(readr) # faster alternatives to base read. methods. 
library(lubridate) # date wrangling. 
library(dplyr) # data wrangling
library(tidyr) # data wrangling
library(ggplot2) # data viz. 
library(pryr) # mem_used() and object_size() functions to manage/understand memory usage.
```
  
  
To read the Part items data and the user webactivity data from s3 I passed read_table2() functions for reading tabular data to read items dataset and the read_csv() function to read in the web user click data CSV files. 

``` {r eval = FALSE, include = FALSE}
# specify personal account keys as environment variables so I can read my s3 object(s) from AWS. 

# DO NOT DO NOT DO NOT DO NOT DO NOT SAVE KEY in code or render in output!!!! Could compromise AWS account. 
#Sys.setenv("AWS_ACCESS_KEY_ID" = "",
#           "AWS_SECRET_ACCESS_KEY" = "")

```

``` {r echo = TRUE}

# items: part number, parent, catalogue, attributes/values.
items <- s3read_using(FUN = read_table2, 
                      object = "obfuscatedItems_10_17_17.txt", 
                      col_names = TRUE,
                      col_types = "cciciiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii",
                      bucket = "pred498team5")

# users: click data from the company website for a random day of user's selected, their activity for past 3 months and click summaries of how they interacted with parts. 

# web user click data from Feb - Mar 2017
users_febmar17 <- s3read_using(FUN = read_csv, 
                      col_names = TRUE,
                      col_types = "ccici",
                      object = "obfuscatedWebActivity7124.csv", 
                      bucket = "pred498team5")

# web user click data from April - June 2017
users_aprmayjun17 <- s3read_using(FUN = read_csv, 
                      col_names = TRUE,
                      col_types = "ccici",
                      object = "obfuscatedWebActivity7127.csv", 
                      bucket = "pred498team5")

# web user click data from July - August 2017
users_julaug17 <- s3read_using(FUN = read_csv, 
                      col_names = TRUE,
                      col_types = "ccici",
                      object = "obfuscatedWebActivity7129.csv", 
                      bucket = "pred498team5")


```
  
  
The 3 separate user click activity queries performed by Matt on his company's graph database were required in order to get around maximum data request requirements. They contain the same information, with the following columns:

``` {r echo = TRUE}
names(users_aprmayjun17)
```
  
  
These 3 datasets of user click activity covering different time periods between February - August 2017 need to be unioned, or combined, into a single dataset. 

``` {r echo = TRUE}

users <- bind_rows(users_febmar17, users_aprmayjun17, users_julaug17)

```
  
  
## Initial Data Wrangling

The **items** dataset contains [obfuscated](https://en.wikipedia.org/wiki/Obfuscation_(software)) information about all of the PartNumbers in the company's inventory, as well as information on the catalog that the PartNumber may have been listed in, and the first 20 attributes and attribute values describing the specifications of each PartNumber. It is a very large dataset with `r nrow(items)` rows, representing the unique PartNumbers.  

The only information we're particularly interested in from the **items** dataset is the Parent column, and left joining that to the matching PartNumber in the users data. Building a recommender system using the Parent family instead of the specific PartNumber will be discussed later. The items dataset has a lot of columns that describe attributes of each PartNumber and Parent. For purposes of a collaborative filtering model, we don't need to know these, any columns that include *"Attr"*, *"Val"*, or *"Catalog"* in its name are dropped. 

``` {r echo = TRUE}
user_items <- users %>%
  left_join(items, by = "PartNumber") %>%
  mutate(ActionDate = ymd(ActionDate), # parse into a date format.
         ActionId_label = factor(ActionId, # create labels
                                 labels = c("add to order", "select Part",
                                            "select Part detail",
                                            "print detail",
                                            "save CAD drawing detail",
                                            "print CAD drawing detail"))) %>% 
  select(-starts_with("Val"), -starts_with("Attr"), -starts_with("Catalog"))
```
  
  
Check that the left join preserved all of the original data in our full users dataset after joining in the Parent value associated with each PartNumber searched by users.

``` {r echo = TRUE}
all.equal(users[1:3], user_items[1:3])
```

Our original datasets are taking up a lot of memory. Specifically, all other datasets other than user_items are currently taking up `r object_size(users, items, users_febmar17, users_aprmayjun17, users_julaug17)`MB of RAM. We'll remove them since everything we need in terms of user click data, the PartNumbers, Parent, actions taken, and the action date are all in the user_items dataset. Our main user_items dataset is quite large, requiring `r object_size(user_items)` MB of RAM. 

``` {r echo = TRUE}
# We can remove the other datasets we don't need anymore to save available RAM. 
remove(users, items, users_febmar17, users_aprmayjun17, users_julaug17)

```
  
  

## Descriptions of the User & Items Data

The user click data (**may refer to users interchangeably as visitors**) came from three separate graph database queries that were required in order to get around maximum data request requirements. Together, the 3 user datasets represent click data from a **random sample** of accounts that purchase from the company's website who had made purchases recently. These visitor's individual  click activities were queried over a time period from `r min(user_items$ActionDate)` through `r max(user_items$ActionDate) `, covering a range of `r max(user_items$ActionDate) - min(user_items$ActionDate)` days. Therefore, these data may provide insight into how users/visitors of a manufacturing supplier are interacting with the company's website, which can form the basis of learning from and providing recommendations on items tailored to each user's behavior and interests to improve the user experience and find items faster. Each variable in the main dataset is defined as follows:

* **VisitorId**: This represents the unique account associated with the customer. It is not based on IP address or other geo-tagging information. The queries set up to curate this dataset were intentionally focused on identifying actual customers with account logins who have purchased from the company in the past. There are `r unique(user_items$VisitorId) %>% length()` unique VisitorIDs in this random sample of user data. 

* **PartNumber**: This represents the ID of the specific product Part Number (ex. a pair of gloves, welding mask, pipe, cable, screw) that the VisitorId interacted with in some way(s) during their web session. 

* **Parent**: The Parent level is the hierarchical category of similar products that every specific PartNumbers roll up to. Within the copmany website, it's a hyperlink associated with the PartNumber. Parent groups will vary in size. It is a way that individual merchandising managers at the company decide to categorize and organize the hundreds of thousands of PartNumbers. 

* **ActionId**: represents the type of click action that occurred. There are `r unique(user_items$ActionId) %>% length()` unique actions that a user can take, which are defined by the next variable for each value of ActionId.

* *ActionId_label*: The `r unique(user_items$ActionId) %>% length()` unique actions for each ActionId value are
    + *1 = add to order*: the user added the PartNumber to their order.
    + *2 = select Part*: the user clicked on the PartNumber to get more information. This is the most basic user action.
    + *3 = select Part detail*: the user selected to drill in for more detail about the PartNumber interacting with.
    + *4 = print detail*: the user selected to print the detail about the PartNumber that they drilled into.
    + *5 = save CAD drawing detail*: the user saved the computer aided draft (CAD) drawing about the PartNumber. 
    + *6 = print CAD drawing detail*: the user printed the computer aided draft (CAD) drawing about the PartNumber. 
  
* **ActionDate**: This is the date of the user's session with a specific PartNumber(s). A user may have had multiple sessions with different PartNumbers in the same day. 

* **ActionCount**: For each ActionId that a user took while interacting with a PartNumber within a session, a count of 1 is recorded. So if visitor X interacted with PartNumber Y today by going back and forth and selecting the same PartNumber three times within this day's session, visitor X's ActionCount for PartNumber Y for today will be 3. 

With a baseline dataset and description of the different variables provided above, we can start describing some information about the user activity. 

Again, the web activity covered by these user clicks on the website begins on `r min(user_items$ActionDate)` and goes through `r max(user_items$ActionDate) `, covering a span of `r max(user_items$ActionDate) - min(user_items$ActionDate)` days. 

### Weekly Web log Activity 

For the `r unique(user_items$VisitorId) %>% length()` unique VisitorIDs in this random sample of user data, it is good to know how these users interacted on the website over time in terms of total clicks, the number of different PartNumbers that are interacted with, and he number of different Parent categories interacted with over fixed intervals of time. 

The line chart shows that engagement in terms of clicks, PartNumbers, Parent categories over time is generally pretty consistent week to week. There are som exceptions. For instance in the week ending 5-14-17, there were only 4 total clicks recorded for 4 different PartNumbers of 4 different Parent categories, which is highly anomalous. Additionally, the week ending July 2, 2017 also saw a noticeable spike in click activity and user interaction with different PartNumbers and Parent categories. This may be because users were planning to be off the following week since 4th of July fell on the following Tuesday. 

Summary statistics are provided after the chart for each of the three different measures. 

``` {r echo = TRUE}
weekly_activity <- user_items %>%
  mutate(week_ending_date = ceiling_date(ActionDate, "week")) %>%
  group_by(week_ending_date) %>%
  summarize(PartNumbers_count = length(unique(PartNumber)), # number of different Partnumbers interacted with.
            Parent_category_count = length(unique(Parent)), # number of different Parents interacted with.
            total_clicks = sum(ActionCount)) %>%
  arrange(week_ending_date)

weekly_activity %>%
  mutate(week_ending_date = as.Date(week_ending_date, "%Y-%m-%d")) %>% 
  gather(key = key, value = value, -week_ending_date) %>%
  ggplot(aes(x = week_ending_date, y = value, group = key)) +
    geom_line(aes(color = key)) +
    scale_x_date(date_breaks = ("1 month"),
               date_labels = c(month.name[1:9])) +
    scale_y_sqrt(breaks = c(1, 50, 2500, 25000, 75000, 200000, 400000, 600000, 800000),
                 labels = function(n) format(n, scientific = FALSE)) + # raw frequency scale
    labs(title = "Web log Activity by Users Over Time",
         subtitle = "broken down weekly",
         x = "2017",
         y = "Count") 
```
  
The number of clicks by this sample of users distributed week over week as indicated by the table below. There was one particularly unusual week around May 14, as previously highlighted.

``` {r echo = TRUE}
library(knitr)
weekly_activity %>%
  summarize(minimum = min(total_clicks),
            First_Quartile = quantile(total_clicks, .25),
            median = median(total_clicks),
            mean = mean(total_clicks),
            Third_Quartile = quantile(total_clicks, .75),
            maximum = max(total_clicks),
            Std_Deviation = sd(total_clicks)) %>%
  kable(caption = 'Typical Count of total clicks by Week', align = 'c')
```

The number of different PartNumbers interacted with each week by users in this dataset distributes as indicated by the following summary statistics. 

``` {r echo = TRUE}
library(knitr)
weekly_activity %>%
  summarize(minimum = min(PartNumbers_count),
            First_Quartile = quantile(PartNumbers_count, .25),
            median = median(PartNumbers_count),
            mean = mean(PartNumbers_count),
            Third_Quartile = quantile(PartNumbers_count, .75),
            maximum = max(PartNumbers_count),
            Std_Deviation = sd(PartNumbers_count)) %>%
  kable(caption = 'PartNumbers interacted with by Users each Week', align = 'c')
```
  
The number of different Parent categories interacted with each week in this dataset distributes as indicated by the following summary statistics.  

``` {r echo = TRUE}
library(knitr)
weekly_activity %>%
  summarize(minimum = min(Parent_category_count),
            First_Quartile = quantile(Parent_category_count, .25),
            median = median(Parent_category_count),
            mean = mean(Parent_category_count),
            Third_Quartile = quantile(Parent_category_count, .75),
            maximum = max(Parent_category_count),
            Std_Deviation = sd(Parent_category_count)) %>%
  kable(caption = 'Parent categories interacted with by Users each Week', align = 'c')
```
  
### Description of Unique Visitors, PartNumbers, Parents

This was described before when defining the variables in the data, but I'll reiterate it here too. Initial analysis shows that there are `r unique(user_items$VisitorId) %>% length()` unique VisitorID numbers in the entire dataset of randomly sampled user activity from `r min(user_items$ActionDate)` through `r max(user_items$ActionDate)`. 

There are `r unique(user_items$PartNumber) %>% length()` unique PartNumbers associated with the visitor web activity data.

Instead of looking at very specific PartNumbers, we can also roll the analysis up to the Parent level, which was defined previously. There are `r unique(user_items$Parent) %>% length()` unique Parent family numbers (referred herein as Parents) associated with the visitor web activity data. 

Code for getting the aforementioned details is below. 

``` {r echo = TRUE, eval = FALSE }
# How many Different Users are in the dataset? 
unique(user_items$VisitorId) %>% length()
# How many distinct PartNumber's are in the dataset? 
unique(user_items$PartNumber) %>% length()
# How many distinct Parent Family's of PartNumbers are in the dataset? 
unique(user_items$Parent) %>% length()

```
  
  
### How do total Action Counts of each visitor distribute? 

Action counts, as previously defined represent how often a visitor did a particular action within their session. In a single session (the day of the visitor activity interaction with a PartNumber) if they selected a PartNumber in the carousel, this is reflected as a single row in the dataset, with an ActionCount of 1. If they selected the part 2 times in this session, the ActionCount value is equal to 2. 

``` {r echo = TRUE}
user_items %>%
  group_by(VisitorId) %>%
  summarize(activity_count = sum(ActionCount)) %>%
  ggplot(aes(activity_count, y = ..density..)) + 
  geom_histogram(binwidth = 100, colour = "black", fill = "darkgrey") +
  # median vertical line.
  geom_vline(aes(xintercept = median(activity_count)),
             color = "black", linetype = "dashed", size = 0.5) +
  geom_text(aes(0,.0015, family = "courier", label = paste("median = ", median(activity_count))),
            nudge_x = 1500, color = "black", size = 4.5) +
  labs(title = 'Distribution of Total ActionCount by each Visitor',
       subtitle = 'binwidth of 100 ',
       x = "Total Actions per Visitor")

```
``` {r}
user_items %>%
  group_by(VisitorId) %>%
  summarize(activity_count = sum(ActionCount)) %>%
  with(summary(activity_count))
```
  
  
After careful consideration, we have determined that total actions per PartNumber or per Parent within a single user session should not influence the implicit rating derived for the recommender. This could be for a variety of reasons, but primarily because there's not a strong theoretical basis that repeating an action in a single session with an item is an endorsement of that item. For example, selecting a part multiple times in the same session may perhaps be just as indicative of uncertainty or confusion as it could be of an interest in the item. 
  
  
### How many different PartNumbers is each visitor interacting with? 


``` {r echo = TRUE}
user_items %>%
  group_by(VisitorId) %>%
  distinct(PartNumber) %>% # only keep distinct part numbers per visitor.
  summarize(unique_part_count = n()) %>% # get count within the group. 
  ggplot(aes(unique_part_count, y = ..density..)) + 
  geom_histogram(binwidth = 100, colour = "darkgray", fill = "chartreuse3") +
  # median vertical line.
  geom_vline(aes(xintercept = median(unique_part_count)),
             color = "black", linetype = "dashed", size = 0.5) +
  geom_text(aes(0,.002, family = "courier", label = paste("median = ", median(unique_part_count))), 
            nudge_x = 600, color = "black", size = 4.5) +
  labs(title = 'How many PartNumbers each Visitor interacted with',
       subtitle = 'binwidth of 100',
       x = 'Different PartNumbers per Visitor')

```

``` {r}
user_items %>%
  group_by(VisitorId) %>%
  distinct(PartNumber) %>% # only keep distinct part numbers per visitor.
  summarize(unique_part_count = n()) %>% # get count within the group. 
  with(summary(unique_part_count))


```
  
  
### How many different Parents are visitors interacting with? 

Instead of looking at the distinct PartNumbers that each visitor interacts with, we can also look at the Parent category that each PartNumber rolls up to. By reducing the diversity of different products from the specific PartNumber to the Parent category they roll up to, we'd expect the count of interactions with unique Parents to decrease. However, this may improve the sparsity problem of a recommender system. 

``` {r echo = TRUE}

user_items %>%
  group_by(VisitorId) %>%
  distinct(Parent) %>% # only keep distinct parent per visitor.
  summarize(unique_parent_count = n()) %>% # get count within the group. 
  ggplot(aes(unique_parent_count, y = ..density..)) + 
  geom_histogram(binwidth = 50, colour = "darkgray", fill = "cadetblue3") +
  # median vertical line.
  geom_vline(aes(xintercept = median(unique_parent_count)),
             color = "black", linetype = "dashed", size = 0.5) +
  geom_text(aes(0,.004, family = "courier", label = paste("median = ", median(unique_parent_count))), 
            nudge_x = 300, color = "black", size = 4.5) +
  labs(title = 'How many Parent categories each Visitor interacted with',
       subtitle = 'binwidth of 50',
       x = 'Parent categories per Visitor')
```

``` {r}
different_parent_per_user_summary <- user_items %>%
  group_by(VisitorId) %>%
  distinct(Parent) %>% # only keep distinct parent per visitor.
  summarize(unique_parent_count = n()) %>% # get count within the group. 
  with(summary(unique_parent_count))

different_parent_per_user_summary
```
  
  
### How many Different Visitors have interacted with each PartNumber? 

The following summary stats distribution is potentially problematic. We have a very high dimensional dataset (more PartNumbers than users). So while each user appears to be interacting with quite a few PartNumbers as indicated by previous graphs, this graph shows that for any given PartNumber, there are typically only a handful of different visitors who have explored the exact same PartNumber. 
``` {r echo = TRUE}
user_items %>%
  group_by(PartNumber) %>%
  distinct(VisitorId) %>% # only keep distinct visitors per Item.
  summarize(unique_users_count = n()) %>%
  mutate(unique_users_count_max500 = ifelse(unique_users_count > 500, 500, unique_users_count)) %>%
  ggplot(aes(unique_users_count_max500, y = ..density..)) + # modal value is around log(2) or 2 items.
  geom_histogram(binwidth = 10, colour = "darkgray", fill = "orangered1") +
  # median vertical line.
  geom_vline(aes(xintercept = median(unique_users_count)),
             color = "black", linetype = "dashed", size = 0.5) +
  geom_text(aes(0,.04, family = "courier", label = paste("median = ", median(unique_users_count))), 
            nudge_x = 60, color = "black", size = 4.5) +
  labs(title = 'Distribution of Number of Different Visitors going each PartNumber',
       subtitle = 'binwidth of 10',
       x = 'Different Visitors per PartNumber (upper limit coerced to 500)')
```

``` {r}
different_visitors_per_partnumber_summary <- user_items %>%
  group_by(PartNumber) %>%
  distinct(VisitorId) %>% # only keep distinct visitors per Item.
  summarize(unique_users_count = n()) %>%
  with(summary(unique_users_count))
different_visitors_per_partnumber_summary

```
  
  
Typically, on median, there are `r different_visitors_per_partnumber_summary[3]` different visitors who've visited the same PartNumber, for the `r unique(user_items$PartNumber) %>% length()` PartNumber categories.
  
  
### How many Different Visitors have interacted with each Parent Family? 

Because Parent Family's are a broader generalization than PartNumbers, we expect to have more interactions within a single Parent Family by multiple users than at the very specific PartNumber level. The summary statistics and distributions provided below reflect that when the dimensionality of the items is rolled up from the granular PartNumber level to the more general Parent category, we get more users who have looked at the same item.  

``` {r echo = TRUE}
user_items %>%
  group_by(Parent) %>%
  distinct(VisitorId) %>% # only keep distinct visitors per Item.
  summarize(unique_users_count = n()) %>%
  mutate(unique_users_count_max2000 = ifelse(unique_users_count > 2000, 2000, unique_users_count)) %>%
  ggplot(aes(unique_users_count_max2000, y = ..density..)) + 
  geom_histogram(binwidth = 50, colour = "darkgray", fill = "orchid") +
  # median vertical line.
  geom_vline(aes(xintercept = median(unique_users_count)),
             color = "black", linetype = "dashed", size = 0.5) +
  geom_text(aes(0, .005, family = "courier", label = paste("median = ", median(unique_users_count))),
            nudge_x = 300, color = "black", size = 4.5) +
  labs(title = 'Distribution of Number of Different Visitors for each Parent Family',
       subtitle = 'binwidth of 50',
       x = 'Different Visitors per Parent (upper limit coerced to 2000)')
```

``` {r}
different_visitors_per_parent_summary <- user_items %>%
  group_by(Parent) %>%
  distinct(VisitorId) %>% # only keep distinct visitors per Item.
  summarize(unique_users_count = n()) %>%
  with(summary(unique_users_count))
different_visitors_per_parent_summary
```
  
  
Typically, on median, there are `r different_visitors_per_parent_summary[3]` different visitors who've visited the same Parent category, of the `r unique(user_items$Parent) %>% length()` Parent categories. 

This represents `r round(different_visitors_per_parent_summary[3] / different_visitors_per_partnumber_summary[3], 1)` times more different visitors per Parent than per PartNumber. Perhaps it may make more sense to model recommendations at the Parent level instead of at the PartNumber level, given these differences in how many unique users are looking at identical PartNumbers versus identical Parent categories. 

However, we see a bit of a presence of fat tail. The max number of different visitors per Parent category is `r different_visitors_per_parent_summary[6]`, which means that for at least 1 outlier parent category, `r round(different_visitors_per_parent_summary[6] / unique(user_items$Parent) %>% length(), 2) * 100`% of different users have interacted with the same parent category. This is important to know in the collaborative filtering setting, as popular items have the potential to heavily influence identification of similar users and items, which can bias new recommendations towards more popular items. 
  
#### Impact of Long Tail Distributions

The previous 2 distributions of the frequency of number of users who rate PartNumbers and Parent categories, respectively both display a unique property common to recommender system problems. They have a very fat right-tail distribution. This means that most items available are rated infrequently by users and there are only a few commonly rated items, or "popular items". 

According to [Aggarwal (2016)](https://www.amazon.com/Recommender-Systems-Textbook-Charu-Aggarwal/dp/3319296574), "The long tail distribution implies that the items, which are frequently rated by users are fewer in number. This fact has important implications for neighborhood-based collaborative filtering algorithms because neighborhoods are often defined on the basis of these frequently rated items. In many cases, the ratings of these high-frequency items are not representative of the low-frequency items because of inherent differences in the rating patterns of the two classes of items" (section 2.2).

Additionally, Aggarwal mentions that because some items are very popular across different users, "such ratings can sometimes worsen the quality of the recommendations becaues they tend to be less discriminative across different users. The negative impact of these recommendations can be experienced both during the peer group computation and also during the prediction computation" (section 2.3.1.4).
  
  
### How many PartNumbers fall into each Parent Family?

It's also interesting to know when we reduce the dimensionality of the products by rolling up from PartNumber to their parent categories, how many PartNumbers make up the Parent? 

``` {r echo = TRUE}
user_items %>%
  group_by(Parent) %>%
  distinct(PartNumber) %>% # remove duplicate part numbers within Parent Group. 
  summarize(unique_partnumber_count = n()) %>% # return count of unique items per parent. 
  with(summary(unique_partnumber_count))
```
  
  
### Do Parents with more PartNumbers also have more Visitors? 

When we roll PartNumbers up to the parent level, the number of different users who interact with a Parent category may be a function of the tally of different PartNumbers that inheret from the Parent. If this is the case, we'd likely see a correlation between the frequency of PartNumbers per Parent and the number of unique visitors per Parent. Collaborative filtering methods are notorious for being biased by the the most popularly rated items, as those ratings will be a large influence in measuring inter-user similarity (user-based collaborative filtering) or inter-item similarity (item-based collaborative filtering). 

``` {r}

parent_unique_partnumbers <- user_items %>%
  group_by(Parent) %>%
  distinct(PartNumber) %>% # remove duplicate part numbers within Parent Group. 
  summarize(unique_partnumber_count = n())

parent_unique_users <- user_items %>%
  group_by(Parent) %>%
  distinct(VisitorId) %>% # only keep distinct visitors per Item.
  summarize(unique_users_count = n()) 

parent_characteristics <- inner_join(parent_unique_partnumbers, parent_unique_users)
remove(parent_unique_partnumbers, parent_unique_users)

parent_characteristics %>%
  ggplot(aes(x = unique_partnumber_count, y = unique_users_count)) +
    geom_point(alpha = 0.5) + 
    stat_smooth(method = "loess", se = TRUE, span = 0.2) +
    labs(title = 'Relationship between unique PartNumbers and unique Users',
         subtitle = 'per Parent category',
         x = 'PartNumbers per Parent',
         y = 'Visitors per Parent')
```
  
However, this may be a necessary trade-off for these types of recommender systems as the more items in common that any pair of users have rated, the more robust the measures of similarity are, from which new recommendations can be made for items. For these algorithms to work well, it is important to adequately measure similar users or items, which is already difficult with very sparse matrices filled with primarily missing entries. 
  
### What types of click Actions do the visitors take? 

Below is the distribution of the 6 different click actions in the visitor click dataset of `r nrow(user_items)` different click actions. 

``` {r echo = TRUE}
# Bar graph of the 6 different Action ID Labels
user_items %>%
  group_by(ActionId, ActionId_label) %>%
  summarize(frequency = n()) %>%
  ggplot(aes(x = reorder(ActionId_label, desc(frequency)), y = frequency / sum(frequency))) +
    geom_bar(aes(fill = ActionId_label), stat = "identity") +
    labs(title = "Click Actions taken by Users",
         subtitle = "on company website",
         x = NULL,
         y = "Percent") +
    geom_text(aes(label = frequency, family = "courier", ), 
              size = 3,
              stat= "identity", 
              vjust = -.5) +
    guides(fill=FALSE) + # remove legend
    theme(axis.text.x=element_text(angle=25,hjust=1)) + # tilt x labels
    scale_y_continuous(labels=scales::percent)

```

``` {r}
user_items %>%
  group_by(ActionId, ActionId_label) %>%
  summarize(frequency = n()) %>%
  ungroup() %>%
  mutate(proportion = round(frequency / sum(frequency), 2)) %>%
  arrange(desc(frequency)) %>%
  kable()
```
  
  
### (TO EXPLORE) What percent of the time is selecting a part a precursor to adding to cart?

If most of the clicks in the dataset reflect that generally, these ultimately lead to adding items to cart, perhaps this is some evidence that customers tend to know what they want when they land on the website. They aren't just querying the site or app out of general interest. 
  
  
## More Data Wrangling: Creating Ratings

Having explored information about the users and the items they've interacted with in the dataset, we need to devise a ratings system derived from user click actions. These user click actions are not explicit ratings but *inferred ratings* because the user is not telling us directly that they like this item. But, the actions themselves have a hierarchy that can be interpreted as different levels of interest, which may enable us to create "implicit ratings". In our company's case, a higher rating would correspond to which actions we would interpret to be closest to a purchase decision or showing a high level of interest. 

This is generally a very popular way of devising ratings for ecommerce because most online interactions by web users aren't explicit ratings by users of a page or product, but simply interactions with that page or product.
  
  
### Convert Actions to Implicit Ratings

After consultation with Matt, the subject matter expert at the company, we decided to convert the values from the **ActionId** column into 3 distinct ratings. Specifically, the 6 different actions a user could take when interacting with distributed into a 1,2,3 ratings system from lowest to highest inferred rating. 

* Rating of 1 (low) = "select Part"

* Rating of 2 = "select Part detail"

* Rating of 3 (high) = "add to order", "print detail", "save CAD drawing detail", or "print CAD drawing detail"

These were coded into our dataset by creating a new variable called **action_rating**, which was derived from the **ActionId_label** of every user interaction. 

``` {r echo = TRUE}
user_items <- user_items %>%
  mutate(action_rating = if_else(ActionId_label == "select Part", 1, 
                                 if_else(ActionId_label == "select Part detail", 2, 3)))

user_items %>%
  group_by(action_rating) %>%
  summarize(frequency = n()) %>%
  ggplot(aes(x = action_rating, y = frequency / sum(frequency))) +
    geom_bar(aes(fill = desc(action_rating)), stat = "identity") +
    labs(title = "Implicit Action Ratings taken by Users",
         subtitle = "on company website",
         x = NULL,
         y = "Percent",
         fill = "Implicit Rating") +
    geom_text(aes(label = frequency, family = "courier", ), 
              size = 3,
              stat= "identity", 
              vjust = -.5) +
    scale_y_continuous(labels=scales::percent)
```

``` {r}
user_items %>%
  group_by(ActionId, ActionId_label, action_rating) %>%
  summarize(frequency = n()) %>%
  ungroup() %>%
  mutate(proportion = round(frequency / sum(frequency), 2)) %>%
  arrange(desc(frequency)) %>%
  kable()
```
  
  
### Creating a weighted Total Rating per item

Having converted the 6 different possible actions into an implicit rating for each of the `r nrow(user_items)` individual user interactions in the user_items dataset, there is still some more pre-processing required of these **action_ratings**.

For this recommender system, recommendations of Parent categories were used instead of a specific PartNumber prediction problem. Doing this helps reduce the high dimensionality of the items, therefore reducing the sparsity problem of few users rating a single PartNumber. It also improves the computation time of the recommender system as there are fewer Parent categories to create predicted recommendations for relative to PartNumbers. 

The weighted rating, which in its final form is called the total_rating was derived in a series of steps:

* **Step 1**: Within each visitor's session (ActionDate) with a Parent category, identify the maximum action_rating the user gave to that Parent from its interaction. 

    In the example below, Visitor #1000368642442 interacted with Parent #M65-364625 in three different sessions (2-22-2017, 6-14-2017, and  6-23-2017). Their interaction with Parent #M65-365145 occurred in one session only on 6-7-2017.    

``` {r}
example_step1 <- user_items %>%
  select(ActionDate, VisitorId, Parent, PartNumber, starts_with("Action"), -ActionCount) %>%
  arrange(VisitorId, Parent, PartNumber, ActionDate) %>%
  slice(1:1000)

example_step1 %>%
  slice(104:115) %>%
  kable(align = 'c')
```
    For each of the sessions Visitor #1000368642442 had per table above, we extract the row of the max   action_rating it gave to each Parent category within the specific session.

``` {r}
example_step1 <- example_step1 %>%
  slice(104:115) %>%
  group_by(VisitorId, Parent, ActionDate) %>%
  # w/in window, keep row(s) of max action_rating for that specific session.
  filter(action_rating == max(action_rating)) %>%
  arrange(VisitorId, Parent, PartNumber, ActionDate) %>%
  ungroup()

example_step1 %>%
  kable(align = 'c')
```
  
  
* **Step 2**: If there are multiple max action_rating interactions per Parent within the same session (ex. the user did an "add to cart" action over multiple PartNumbers in a session, all inheriting from the same Parent category), we will sum these max action_rating values up within that session to get a session_action_rating. The intuition here is that the more PartNumbers in a specific Parent category the user interacted with in a single session is a higher inferred endorsement of that Parent. 

    Back to our example. Visitor #1000368642442 did in fact interact with multiple different PartNumbers of  the same Parent #M65-364625 within a couple specific sessions (6-14-2017 and 6-23-2017). Hence multiple rows for those dates as illustrated above. Per the described weighting method, we sum up the max action_rating values within the session (ActionDate) and Parent category to yield Visitor #1000368642442's session_action_rating with a Parent. 

``` {r}
example_step2 <- example_step1 %>%
  group_by(VisitorId, Parent, ActionDate) %>%
  # Step2
  summarize(session_action_rating = sum(action_rating)) %>%
  arrange(VisitorId, Parent, ActionDate) %>%
  ungroup()

example_step2 %>%
  kable(align = 'c')


```
  
  
* **Step 3**: Factor in the different number of sessions over time each user has engaged with each Parent category by adding up each user's maximum action_rating for a Parent category across all their different sessions between `r min(user_items$ActionDate)` and `r max(user_items$ActionDate)`.

    Back to our example for Visitor #1000368642442. They have 3 different session_action_rating with Parent #M65-364625. We will sum up their session_action_ratings to get a total_rating of 15 for Parent #M65-364625 For the other Parent #M65-365145, this visitor only interacted with this during a single session, so it's total_rating will equal its session_actiong_rating of 3. 

``` {r}
example_step3 <- example_step2 %>%
  # Step3
  group_by(VisitorId, Parent) %>%
  summarize(total_rating = sum(session_action_rating)) %>%
  arrange(VisitorId, Parent) %>%
  ungroup()

example_step3 %>%
  kable(align = 'c')
```
  
  
* **Step 4**: After weighting the action_rating per user for each Parent by considering diversity of the PartNumbers of a Parent they interacted with per session and the total number of sessions they interacted with the Parent over time, the ratings need to be transformed to reduce extreme outliers. 

    For our example for Visitor #1000368642442, the different total_ratings could look something like this.

``` {r}
example_step3 %>%
  mutate(total_rating_sqrt = sqrt(total_rating),
         total_rating_logn = log(total_rating),
         total_rating_max10 = ifelse(total_rating > 10, 10, total_rating)) %>%
  kable(align = 'c')
```
  
  
The actual code for the preprocessing of Steps 1 through 4 proceeds below via use of a `dplyr` analysis pipeline:

``` {r echo = TRUE}

user_ratings <- user_items %>%
  select(VisitorId, Parent, ActionDate, action_rating) %>%
  # create group window for each Visitor's individual session(day) with a Parent# 
  group_by(VisitorId, Parent, ActionDate) %>%
  # Step1: w/in window, keep row(s) of max action_rating for that specific session.
  filter(action_rating == max(action_rating)) %>%
  # Step2: sum up all max action_ratings to account for >1 max actions with diff. 
  # PartNumbers of that Parent# during session. An additive weight w/in session of a Parent#.
  summarize(session_action_rating = sum(action_rating)) %>%
  # change group window to roll up next aggregations to Visitor:Parent, across all sessions. 
  group_by(VisitorId, Parent) %>%
  # Step3: per Parent per Visitor, sum the session_action_rating across all sessions (days).
  summarize(total_rating = sum(session_action_rating)) %>%
  ungroup() %>%
  # Step4: ratings transformation candidates
  mutate(total_rating_sqrt = sqrt(total_rating),
         total_rating_logn = log(total_rating),
         total_rating_max10 = ifelse(total_rating > 10, 10, total_rating)) %>%
  arrange(VisitorId, total_rating)

```
  
  
### Selecting an Appropriate Scale Transformation for Total_rating.

Came up with 3 initial different ways to transform the total_rating per item (the Parent category) in order to handle the large outliers in the raw total_rating distribution. The 3 other transformations are a lognormal transformation, square root transformation, and coercing the upper limit to a max value of 10. These are visualized below alongside the original total_rating.

``` {r echo = TRUE}
tidy_ratings <- user_ratings %>%
  gather(key = rating_type, value = value, -VisitorId, -Parent)

tidy_ratings %>%
  ggplot(aes(value, fill = rating_type)) +
  geom_histogram(data = filter(tidy_ratings, rating_type == "total_rating"), binwidth = 20, color = "black") +
  geom_histogram(data = filter(tidy_ratings, rating_type != "total_rating"), binwidth = 1, color = "black") + 
  facet_wrap(~ rating_type, scales = "free") +
  labs(title = "Distribution of Visitors' Total Rating of Parent Part Families",
       subtitle = "with different scale transformations",
       x = NULL) +
  guides(fill = FALSE) +
  scale_y_continuous(labels = function(n) format(n, scientific = FALSE)) # raw frequency scale


remove(tidy_ratings)
```
  
Below are the standard summary stats below for each of the 4 different ratings methods. 

``` {r}
summary(user_ratings[3:6])
```

When applying a sqrt() or log() transformation, the total_ratings preserve their rank order but we also get the effect that the original total_ratings that were not outliers originally become much closer to each other. We can see this effect by observing the middle 50% range of values, or interquartile range (IQR), in the output below. There's a much tighter range of values for the scale transformed ratings (square-root and lognormal), whereas simply coercing the outliers to an upper boundary like total_rating_max10, still preserves the variance in the middle 50% range of ratings values as the original, but highly skewed total_rating. 

``` {r}
library(purrr)
map_dfr(user_ratings[3:6], IQR) # calculate the IQR for each total rating method
```


We can also look at the standard deviation for each of the 4 different total_ratings too. Naturally, the raw total_ratings will have the largest standardized variance b/c of its extreme right-skew. The standard variance of each total rating reflects the same insights from the IQR values. Total_rating_max10 seems to balance keeping the scores in a reasonable range and handling outliers while still preserving a lot of the variance  between any item rating. 

Total_rating_max10 will be what I use initially to explore collaborative filtering. 

``` {r echo = TRUE}
map_dfr(user_ratings[3:6], sd) # get the standard deviation for each total tating method
```

### Setting up a User Ratings Matrix

We're at the point now where we have a dataset where each row represents the total_rating of a specific item (Parent) made by a specific VisitorID. A reminder of what these primary columns of interest look like in our current dataset:

``` {r echo = TRUE}
sample_n(user_ratings[,c(1:2, 6)], 20) %>%
  kable(align = 'c')

```
  
These 3 components are all that is needed to craete a user-items rating matrix. The rows of the matrix will represent individual users, columns will represent the distinct Parent categories, and the elements in the matrix will represent the respective total_rating of the item. 

To make the storage of the matrix memory efficient, a sparse matrix will be used. A sparse matrix, using a package like `matrix` only fills the non-missing entries in the matrix, making them [very memory efficient](http://www.johnmyleswhite.com/notebook/2011/10/31/using-sparse-matrices-in-r/) for large matrix objects.

The row (i) and column (j) indices of a matrix need to be 1 starting index based. The easiest way to facilitate this while still being able to map the row and column labels is to coerce the VisitorId and Parent columns in our dataset to as.factor() and then to as.integer(). When coercing to a factor, the values will be alphabetically sorted by default. We can then pass dimension names to the row and columns of the matrix by passing the unique, sorted VisitorId and Parent labels. See code below for details. 

```{r echo = TRUE}
library(Matrix)
# sparse ratings matrix.
# https://stackoverflow.com/questions/28430674/create-sparse-matrix-from-data-frame?noredirect=1&lq=1
# the VisitorId and Parent need to be 1 based indices when creating a matrix. 
# Per ?factor, the levels of a factor are by default sorted.
sparse_r <- sparseMatrix(i = as.integer(as.factor(user_ratings$VisitorId)), 
                         j = as.integer(as.factor(user_ratings$Parent)),
                         x = user_ratings$total_rating_max10)

# can rename the matrix row and column labels with unique VisitorId and Parent names. 
dimnames(sparse_r) <- list(sort(unique(user_ratings$VisitorId)),
                           sort(unique(user_ratings$Parent)))
```

We can check that the levels of the coerced factor are identical name position matches to the dimnames that we passed as labels to the rows and columns of the sparse matrix. 

``` {r echo = TRUE}
all.equal(levels(as.factor(user_ratings$VisitorId)), 
          sort(unique(user_ratings$VisitorId))) 

all.equal(levels(as.factor(user_ratings$Parent)), 
          sort(unique(user_ratings$Parent))) 

# class(sparse_r)
# dim(sparse_r)
# attributes(sparse_r)
# str(sparse_r) # it's an S4 object. use slots (@) to access elements. 

```
  


## Collaborative Filtering Algorithms

According to Aggawal (2016), collaborative filtering models use the "power of the ratings provided by multiple users to make recommendations" (1.3.1). By finding out how users are similar to each other based on how they may rate the same items in a correlated way, or how items may be rated in a correlated way across different users, an algorithm can leverage these correlations or measures of similarity as the basis to predict and recommend new items not yet rated by an indvidual.

For collaborative filtering recommender systems, the recommendations are largely based on the ability to identify recommendations by

* a) finding similar users who've rated the same items similarly to me. This method addresses a recommendation hypothesis of "what other users are like me, and what items do they purchase?" This is known as **user-based collaborative filtering**. It finds users who have similar interest in items as you and recommends new items based on a weighted formula of what they rate highly that you haven't yet looked at or purchased. 

* b) We can also make recommendations by finding items that are similarly rated to the ones that a user has rated. In this case, neighborhoods are defined not by similar users but by items with similar ratings pattern across users. These item similarity measures are then compared to the items the user has already rated. This is known as **item-based collaborative filtering**. It addresses a recommendation hypothesis of "people who liked this also tend to like this" or "What items are most similar to the ones that Iâ€™ve purchased that may interest me?" 
  

## recommenderlab

Since we'll be working initially with action ratings, and not binary ratings (1,0), we'll use the `realRatingMatrix` class of objects from `recommenderlab`. Some of the methods we can apply to objects of this class are output below. 

``` {r echo = TRUE}
library(recommenderlab)
methods(class = "realRatingMatrix")

```
  
For now, we'll primarily need to convert our sparse matrix, **sparse_r** into a realRatingMatrix object, which is a pre-processing step for building recommender systems with this package. 

``` {r echo = TRUE}
# coerce our sparseMatrix, sparse_r into a realRatingMatrix object
real_r <- as(sparse_r, "realRatingMatrix")
real_r

real_r_non_missing_percent <- round(sum(rowCounts(real_r)) / (nrow(real_r) * ncol(real_r)), 5) * 100
# str(real_r)
# attributes(real_r)
```

With `r nrow(real_r) * ncol(real_r)` total cells in this realRatingMatrix, and only `r sum(rowCounts(real_r))` total ratings within these cells, the percent of non-missing entries in our matrix is `r real_r_non_missing_percent`%. Therefore, the sparsity of the matrix is `r 100 - real_r_non_missing_percent`%. 

A preview of what this realRatingMatrix looks like for a handful of random rows (users) and columns (items). We see no ratings for this small slice of the matrix. 

``` {r echo = TRUE}
getRatingMatrix(real_r[1:10, 1:5])

```
  
## Improving the Sparsity Problem

Some users and items in this dataset are primary culprits contributing the sparsity in our matrix, such as users who maybe only rated one item in the entire 7-month period of the dataset. 

From a practical perspective, it probably does not makes sense to keep users from this matrix who have close to 0 interactions with items. 

From a modeling perspective, it will be hard to learn from these sparse user vectors and almost impossible to use them as part of any holdout dataset because there will not be enough item ratings to both learn their similarity to other users or items from and also enough remaining item ratings to use for predicting test error. This is where some of our prior EDA will help us out. Specifically, the EDA plots and summary stats sections of this analysis that looked at the questions of "How many different Parents are visitors interacting with?" and "How many Different Visitors have interacted with each Parent Family?" will help us to subset the realRatingMatrix. This will enable us to arrive at a viable reference population for training and testing candidate collaborative filtering models. 

Based on the EDA, we'll use 1st quartile (25th percentile) values as cutoffs for getting into the realRatingMatrix for fitting and evaluating a model. Specifically:

* **User threshold**: The user has to have had at least `r different_parent_per_user_summary[2]` ratings of different items (Parents). This reflects the 25th percentile of user interactions with different Parent categories. 

* **Item threshold**: The item has to have had at least `r different_visitors_per_parent_summary[2]` different users who have interacted with that item (Parent). This reflects the 25th percentile of the number of different visitors per parent category. 

We can simply use matrix subsetting using the rowCounts() and colCounts() functions within recommenderLab to apply these rules for sparsity reduction and ensuring that each user has enough test items available for learning the model and evaluating predicted top N items. Let's look at how many users were dropped as a result of this. 

``` {r echo =TRUE}
real_r_filtered <- real_r[rowCounts(real_r) >= different_parent_per_user_summary[2], ] # thirty
nrow(real_r_filtered) - nrow(real_r)
summary(rowCounts(real_r_filtered))

```
  
As a result of this, we have `r nrow(real_r_filtered) - nrow(real_r)` fewer users in the matrix. 
  
Now also apply the subsetting rules for items.

``` {r echo = TRUE}
real_r_filtered <- real_r_filtered[, colCounts(real_r_filtered) >= different_visitors_per_parent_summary[2],] # eighteen

summary(colCounts(real_r_filtered))

```
  
As a result of this, we have `r ncol(real_r_filtered) - ncol(real_r)` fewer items in the dataset. The new dimensions of the updated realRatingMatrix are:

``` {r}
real_r_filtered_non_missing_percent <- round(sum(rowCounts(real_r_filtered)) / (nrow(real_r_filtered) * ncol(real_r_filtered)), 5) * 100

real_r_filtered
class(real_r_filtered)
```

With `r nrow(real_r_filtered) * ncol(real_r_filtered)` total cells in this realRatingMatrix, and `r sum(rowCounts(real_r_filtered))` total ratings within these cells, the percent of non-missing entries in our reduced matrix is `r real_r_filtered_non_missing_percent`%. We improved our non-missing entries by about `r round((real_r_filtered_non_missing_percent - real_r_non_missing_percent) / real_r_non_missing_percent, 2) * 100`%, although our missingness is still very high. 

## Sample the data

To improve computation time for this very large matrix, we'll take a random sample of 50% of the user rows in the realRatingMatrix.

``` {r echo = TRUE}
set.seed(2017)
keep_index <- sample(seq_len(nrow(real_r_filtered)), size = nrow(real_r_filtered) * 0.5, replace = FALSE)
real_r_filtered_sample <- real_r_filtered[keep_index, ]

```

## User Based Collaborative Filtering
  

### How the UBCF algorithm works

1. Using [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity), figure out how similar each user *i* is to each other. Can also use [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). Jaccard similarity should only be used for binary rating recommenders.
  i) for each user, identify the *k* most similar users to user *i*
2. Per each item *j*, average the ratings given by each user *i*'s nearest *k* users for each item that's not yet been rated by user *i*.
  i) weight these imputed/predicted average ratings based on closest similarity score of each nearest *k* user to user *i* from the items they've already rated in common.
3. Select a Top-N recommendations threshold. 
  

### Create a train and test modeling scheme

Here, we'll train 5 different user-based collaborative filtering models, each with a different set of parameters. The model will be evaluated based on the classification accuracy (ex. True Positive Rate, False Positive Rate) for different top-N recommended items in the test set. For now, we'll traing the recommender algorithm using a simple training and test sample with a holdout sample of 20% of the users. k-fold cross-validation would be ideal, but also computationally more expensive. 


``` {r eval = FALSE, echo = TRUE}

set.seed(2017)
ubcf_test_scheme <- evaluationScheme(real_r_filtered_sample,
                               method = "split", # random train/test scheme
                               train = 0.75,
                               k = 1,
                               given = 10,  # how many records from test user to learn the model?
                               goodRating = 3) # threshold for classification. Median Total_rating_max10.

ubcf_test_scheme

```
  

### Tuning Parameters for UBCF

The different hyper-parameters that need to be specified for the UBCF algorithm are available in the model registry within `recommenderlab`. We'll focus here on the **UBCF_realRatingMatrix** related algorithm. Brief descriptions of this algorithm and its default tuning parameters are illustrated below. 

``` {r echo = TRUE}
# recommenderRegistry$get_entries(dataType = "realRatingMatrix")
recommenderRegistry$get_entries(dataType = "realRatingMatrix")$UBCF_realRatingMatrix 


```

Details about these 4 tuning parameters to account for with this recommender type:

* **method**: this is the type of similarity metric to calculate similarity between users real ratings profile. Cosine similarity, Pearson correlation coefficient, and Jaccard similarity are available options. The first two are not good options if using unary ratings.

* **nn**: this parameter sets the neighborhood of most similar users to consider for each user. The ratings profiles of the *k* nearest neighbors will be the basis for identifying what other users are similar to user *i*, and making new recommendations based on items user *i* has not yet rated that are liked by the *k* nearest neighbors.

* **sample**: a logical value to indicate whether the data should be sampled for train/test.

* **normalize**: how to normalize real ratings provided by different users. This is important because we generally should try to account for individuals behavioral biases by making sure that all ratings are scaled similarly. This can be done by passing a value to this parameter inside the algorithm or applied to the matrix before any modeling too. See `?normalize` for additional details. Zero mean centering will be used, where each user's row vector of ratings is subtracted by its row mean, centering each user's mean at zero. Z-scoring is an alternative method available too that additionally divides each user's rating by its standard deviation. 
  
### Create list of UBCF Models
  
Next, we need to specify the different UBCF algorithms with specified hyper-parameters we want to pass different values for. For now, we'll focus on tuning 2 different similarity **methods** of "cosine" and "pearson". We'll also tune 2 different values of *nn* to start. We'll normalize the ratings per user for all of them. Again, even though these are implicit ratings based on click activity and each user *i*'s max weighted action with each Parent category, each user *i* has their own specific pattern of behavior per session over time, which we may inject a personal behavior bias we want to account for.

``` {r eval = FALSE, echo = TRUE}

ubcf_algorithms <- list(
  "ubcf_cosine_10nn" = list(name = "UBCF",
                            param = list(method = "cosine", 
                                         nn = 10,
                                         normalize = "center")),
  "ubcf_cosine_25nn" = list(name = "UBCF",
                            param = list(method = "cosine", 
                                         nn = 25,
                                         normalize = "center")),
  "ubcf_cosine_50nn" = list(name = "UBCF",
                            param = list(method = "cosine", 
                                         nn = 50,
                                         normalize = "center")),
  "ubcf_pearson_25nn" = list(name = "UBCF",
                            param = list(method = "pearson", 
                                         nn = 25,
                                         normalize = "center")),
  "ubcf_pearson_50nn" = list(name = "UBCF",
                            param = list(method = "pearson", 
                                         nn = 50,
                                         normalize = "center"))
)

```

### Run the UBCF Models

Time to fit the recommender models. The type of model will be a "TopNList" recommender system instead of a "rating" based recommender system. This will allow us to evaluate models on the test data in a classification framework.

``` {r echo = TRUE}

# Model1: centered cosine similarity, identifying 10 nearest neighbors.
ubcf_model1 <- Recommender(data = getData(ubcf_test_scheme, "train"),
                           method = "UBCF", 
                           parameter = ubcf_algorithms$ubcf_cosine_10nn$param)

```

### Predict on test dataset

``` {r echo = TRUE}

ubcf_model1_pred <- predict(ubcf_model1, getData(ubcf_test_scheme, "known"), type = "topNList", n = 10) # top 10 items recommended
s3save(ubcf_model1_pred, bucket = "pred498team5", object = "ubcf_model1_predRdata")
#s3load(model1_pred, bucket = "pred498team5", object = "model1_pred.Rdata")

calcPredictionAccuracy(ubcf_model1_pred, getData(ubcf_test_scheme, "unknown"), given = 10, goodRating = 3) 

```
## Item Based Collaborative Filtering

``` {r echo = TRUE}
# recommenderRegistry$get_entries(dataType = "realRatingMatrix")
recommenderRegistry$get_entries(dataType = "realRatingMatrix")$IBCF_realRatingMatrix 


```
